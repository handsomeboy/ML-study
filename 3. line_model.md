## 本章概要 ##本章讲述了模型评估与选择（model evaluation and selection）的相关知识：2.1 经验误差与过拟合（empirical error & overfitting）> 精度accuracy、训练误差（经验误差）training error（empirical error）、**泛化误差**generalization error、**过拟合**overfitting、欠拟合underfitting；2.2 模型评估方法（evaluate method）> 测试误差testing error、留出法hold-out、分层采样stratified sampling、交叉验证法cross validation、**k-折交叉验证**k-fold cross validation、留一法leave-one-out（LOO）、**自助法**bootstrapping、自助采样bootstrap sampling、包外估计out-of-bag estimate、**调参**parameter tuning、验证集validation set；2.3 模型性能度量（performance measure）> 错误率error rate、查准率（准确率）precision、查全率（召回率）recall、P-R曲线、平衡点BEP、**F1/Fβ**、**混淆矩阵**、**ROC曲线**、AUC、代价敏感cost-sensitive、**代价矩阵**cost matrix、代价曲线cost curve、期望总体代价；2.4 模型比较检验（comparation & testing）> 假设检验hypothesis test、拒绝假设、t-检验t-test、Friedman检验、后续检验post-hoc test、Friedman检验图；2.5 偏差与方差（bias & variance）> 偏差-方差窘境bias-variance dilemma；## 知识梳理### 基本形式              * 线性模型试图学得一个通过属性组合来进行预测的函数 * f(x) = w1*x1 + ... + wd*xd + b    * w表示学习到的权重， b表示学习到的偏置    * xi表示一个样例的属性    * 线性模型具有良好的可解释性                  ### 线性回归* 线性回归中， 最小二乘法就是试图找到一条直线，使所有的样本到直线的欧式距离之和最小### 对数几率回归 logistics function* [Logistic Regression(逻辑回归)详细讲解](https://blog.csdn.net/joshly/article/details/50494548)* [极大似然估计详解](https://blog.csdn.net/zengxiantao1994/article/details/72787849)    * 在固定输入下，估计参数    * 参数是什么的时候，输出最有可能是给定的值，这些参数就是我们要的* 如要将回归方法应用在分类任务上怎么办？* y = 1 / (1 + e^-z)* 是一种 sigmoid 函数* 优点    * 是一种分类学习方法    * 直接对分类可能性进行建模，无需事先假设数据的分布，这样避免了假设分布不准确带来的问题    * 不仅预测出类别，还得到了近似概率预测，    * 函数任意阶可导凸函数## 手写版笔记![](./linear_model/3.1.jpg)![](./linear_model/3.2.jpg)![](./linear_model/3.3.jpg)## 习题解答 ###### 3.1 线性回归模型偏置项 ####> ![](./linear_model/Ch3/3.1.png)偏置项b在数值上代表了自变量取0时，因变量的取值；1.当讨论变量x对结果y的影响，不用考虑b；2.可以用变量归一化（max-min或z-score）来消除偏置。----#### 3.2 证明对数似然函数是凸函数（参数存在最优解） ####> ![](./linear_model/Ch3/3.2.png)直接给出证明结果如下图：> ![](./linear_model/Ch3/3.2.1.png)----#### 3.3 编程实现对率回归 ####> ![](./linear_model/Ch3/3.3.png)所使用的数据集如下：> ![](./linear_model/Ch3/3.3.1.png)本题是本书的第一个编程练习，采用了自己编程实现和调用sklearn库函数两种不同的方式（[查看完整代码](https://github.com/PY131/Machine-Learning_ZhouZhihua/tree/master/ch3_linear_model/3.3_logistic_regression_watermelon/)）：具体的实现过程见：[周志华《机器学习》课后习题解答系列（四）：Ch3.3 - 编程实现对率回归](http://blog.csdn.net/snoopy_yuan/article/details/63684219)----#### 3.4 比较k折交叉验证法与留一法 ####> ![](./linear_model/Ch3/3.4.png)本题采用UCI中的[iris data set](http://archive.ics.uci.edu/ml/datasets/Iris) 和 [Blood Transfusion Service Center Data Set](http://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center) 数据集，借助sklearn实现（[查看完整代码](https://github.com/PY131/Machine-Learning_ZhouZhihua/tree/master/ch3_linear_model/3.4_cross_validation)）。具体的实现过程见：[周志华《机器学习》课后习题解答系列（四）：Ch3 - 3.4.交叉验证法练习](http://blog.csdn.net/snoopy_yuan/article/details/64131129)----#### 3.5 编程实现线性判别分析 ####> ![](./linear_model/Ch3/3.5.png)本题采用题3.3的西瓜数据集，采用基于sklearn实现和自己独立编程实现两种方式（[查看完整代码](https://github.com/PY131/Machine-Learning_ZhouZhihua/tree/master/ch3_linear_model/3.5_LDA)）。具体的实现过程见：[周志华《机器学习》课后习题解答系列（四）：Ch3 - 3.5.编程实现线性判别分析](http://blog.csdn.net/snoopy_yuan/article/details/64443841)----#### 3.6 线性判别分析的非线性拓展思考 ####> ![](./linear_model/Ch3/3.6.png)给出两种思路：- 参考书p57，采用**广义线性模型**，如 y-> ln(y)。- 参考书p137，采用**核方法**将非线性特征空间隐式映射到线性空间，得到**KLDA**（核线性判别分析）。----#### 3.7 最优ECOC编码方式 ####> ![](./linear_model/Ch3/3.7.png)参考书p65，*对于同等长度的编码，理论上来说，任意两个类别间的编码距离越远，纠错能力越强*。那么如何实现呢，可参考文献[Error-Correcting Output Codes](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/lecture_notes/ecoc/ecoc.pdf)。下图是截取文中的关于在较少类时采用**exhaustive codes**来生成最优ECOC二元码的过程：> ![](./linear_model/Ch3/3.7.1.png)采用文中方法，每两类的Hamming Distance均达到了码长的一半，这也是最优的编码方式之一。----#### 3.9 多分类到二分类分解、类别不平衡 ####> ![](./linear_model/Ch3/3.9.png)参考书p66，*对OvR、MvM来说，由于对每类进行了相同的处理，其拆解出的二分类任务中类别不平衡的影响会相互抵销，因此通常不需专门处理。*以**OvR**（一对其余）为例，由于其每次以一个类为正其余为反（参考书p63），共训练出N个分类器，在这一过程中，类别不平衡由O的遍历而抵消掉。----